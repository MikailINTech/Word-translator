{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_translator.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9zi0DQ1NaM9"
      },
      "source": [
        "Implementation based on the article [Exploiting Similarities among Languages for Machine Translation](https://https://arxiv.org/abs/1309.4168)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X0hRPZQNwrd"
      },
      "source": [
        "#### **IMPORTS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYlmmNJadUHi"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.nn import functional as F\n",
        "from sklearn.neighbors import NearestNeighbors as NN\n",
        "from torch import nn\n",
        "from numpy.linalg import svd\n",
        "from sklearn.decomposition import PCA\n",
        "from torchtext.vocab import FastText\n",
        "from sklearn.model_selection import train_test_split \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEC0F_0UgxHC"
      },
      "source": [
        "# For google colab\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "%cd drive/MyDrive/A2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl5i1JHkEEp-"
      },
      "source": [
        "%cd ./drive/MyDrive/Projet word embeddings/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7WYkbizOJow"
      },
      "source": [
        "#### **Download FastText vocabulary (word embeddings)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASTILCfmjPBN"
      },
      "source": [
        "try: \n",
        "  target_vocabulary = FastText('multi.fr', max_vectors=30000)\n",
        "  source_vocabulary = FastText('multi.en', max_vectors=30000)\n",
        "except:\n",
        "  target_vocabulary = FastText('fr', max_vectors=10000)\n",
        "  source_vocabulary = FastText('en', max_vectors=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB9jtDUubS95"
      },
      "source": [
        "dictionary=pd.read_csv(\"en-fr.txt\",sep=\" \", header=None)\n",
        "dictionary.columns=['source','target']\n",
        "dictionary.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzOY0z2bOh1I"
      },
      "source": [
        "#### **Define functions:**\n",
        "\n",
        "- `get_vector`: return embedding vector of given word in a given vocabulary\n",
        "- `closest_words`: among all the words in the vocabulary, find closest ones to a given vector, which is not necessarily in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFhugLghkT29"
      },
      "source": [
        "def get_vector(embeddings, word):\n",
        "  if word not in embeddings.stoi:\n",
        "    print(word, ' not in vocabulary!')\n",
        "    return\n",
        "  return embeddings.vectors[embeddings.stoi[word]]\n",
        "\n",
        "def closest_words(embeddings, vector, n = 10):\n",
        "  vector = torch.as_tensor(vector) if isinstance(vector, (np.ndarray, list)) else vector\n",
        "  if not isinstance(vector, torch.Tensor):\n",
        "     print('vector of type ', type(vector), ' not accepted!') \n",
        "     return\n",
        "  vector_repeated = vector.repeat(embeddings.vectors.shape[0], 1)\n",
        "  distances = torch.norm(vector_repeated - embeddings.vectors, dim=1)\n",
        "  if n==1:\n",
        "    argmin = torch.argmin(distances)\n",
        "    return (embeddings.itos[argmin], distances[argmin].item())\n",
        "  \n",
        "  idxs = sorted(range(len(distances)), key=lambda k: distances[k])[:n]\n",
        "  return [(embeddings.itos[idx], distances[idx].item()) for idx in idxs]\n",
        "\n",
        "def closest_words_cosin(embeddings, vector, n = 10):\n",
        "  vector = torch.as_tensor(vector) if isinstance(vector, (np.ndarray, list)) else vector\n",
        "  if not isinstance(vector, torch.Tensor):\n",
        "     print('vector of type ', type(vector), ' not accepted!') \n",
        "     return\n",
        "\n",
        "  distances = torch.FloatTensor([1 - vector @ i.T for i in embeddings.vectors])\n",
        "  if n==1:\n",
        "    argmin = torch.argmin(distances)\n",
        "    return (embeddings.itos[argmin], distances[argmin].item())\n",
        "  \n",
        "  idxs = sorted(range(len(distances)), key=lambda k: distances[k])[:n]\n",
        "  return [(embeddings.itos[idx], distances[idx].item()) for idx in idxs]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm7fP761klgH"
      },
      "source": [
        "closest_words(target_vocabulary, get_vector(target_vocabulary, 'roi') - get_vector(target_vocabulary, 'homme') + get_vector(target_vocabulary, 'femme'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMbI85e_zO56"
      },
      "source": [
        "closest_words_cosin(target_vocabulary, get_vector(target_vocabulary, 'roi') - get_vector(target_vocabulary, 'homme') + get_vector(target_vocabulary, 'femme'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX9A9P3ensn0"
      },
      "source": [
        "#### **Get index map translations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acChK_-N02t6"
      },
      "source": [
        "stoi_dict_target=pd.DataFrame.from_dict(target_vocabulary.stoi,orient='index')\n",
        "stoi_dict_source=pd.DataFrame.from_dict(source_vocabulary.stoi,orient='index')\n",
        "\n",
        "stoi_dict_target.columns=['index_target']\n",
        "stoi_dict_source.columns=['index_source']\n",
        "stoi_dict_target.index.name='word'\n",
        "stoi_dict_source.index.name='word'\n",
        "stoi_dict_target.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHkn-1EtaQzr"
      },
      "source": [
        "words_indices=dictionary.join(stoi_dict_source,on='source')\n",
        "words_indices=words_indices.join(stoi_dict_target,on='target')\n",
        "\n",
        "words_indices=words_indices.dropna()\n",
        "\n",
        "words_indices['index_target'] = words_indices['index_target'].astype(int)\n",
        "words_indices['index_source'] = words_indices['index_source'].astype(int)\n",
        "\n",
        "words_indices.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efQ091q2VYFP"
      },
      "source": [
        "## TRANSLATE\n",
        "\n",
        "#### **Split train and test**\n",
        "\n",
        "Our data is composed by the indices of each word. We'll next divide it into train and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10ObSYGBth0O"
      },
      "source": [
        "data = words_indices[['index_source', 'index_target']].to_numpy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[:, 0], data[:, 1], test_size=0.2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gre7E8TPEYG4"
      },
      "source": [
        "\n",
        "#### **Nearest neighbors**\n",
        "\n",
        "To compute the nearest neighbors after the training, we'll use `sklearn.nearest_neighbors`. Let's fit our target space into this algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvDKQiUqG_e2"
      },
      "source": [
        "k_s=[1, 2, 5, 8, 10, 15] # accuracies will be computed with respect to many values of neighbors\n",
        "neighbors_tree = NN(n_neighbors=k_s[-1], algorithm='ball_tree')\n",
        "neighbors_tree.fit(target_vocabulary.vectors.numpy())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxEgQLHIVvHc"
      },
      "source": [
        "#### **Define loss and gradient**\n",
        "\n",
        "\n",
        "\n",
        "> Loss\n",
        "\n",
        "Our optimization problem is defined as follows:\n",
        "\n",
        "$$\n",
        "min_w \\sum_{i} ||Wx_i - yi||^2 \n",
        "$$\n",
        "\n",
        "Where each word (french and english, respectively) $x_i, y_i \\in \\mathbb{R}^d$, $d$ being the embeddings dimension.\n",
        "\n",
        "\n",
        "\n",
        "> Gradient\n",
        "\n",
        "\n",
        "Using minibatch gradient descent, our iterations will perform the following step on the matrix $W$:\n",
        "\n",
        "$$\n",
        "W = W - \\dfrac{\\eta}{b} \\sum_{i=1}^{b} \\nabla_i  ||Wx_i - yi||^2 \n",
        "$$\n",
        "\n",
        "Where $\\eta$ is the learning rate and $b$ is the batch size. This can then be calculated as:\n",
        "\n",
        "$$\n",
        "W = W - \\dfrac{2\\eta}{b} (WX - Y)X^T \\,\\,\\,\\,\\, X,Y \\in \\mathbb{R}^{dxb}\n",
        "$$\n",
        "\n",
        "$X$ and $Y$ here are the matrices having, in each column, the distributed representation for the corresponding words of the mini-batch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv2qUzhixtAZ"
      },
      "source": [
        "batch_size=128\n",
        "learning_rate=0.3\n",
        "W = torch.ones((300, 300))\n",
        "\n",
        "def loss(W, X, y, bs):\n",
        "  return torch.norm(W @ X - y)/bs\n",
        "\n",
        "def grad(W, X, y, bs):\n",
        "  return 2*(((W@X - y)) @ X.T)/bs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Spd4SSElJ8NY"
      },
      "source": [
        "#### **Orthogonal case**\n",
        "\n",
        "The orthogonal constraint of the optimization criterium leads to the Procustes problem, which can be solved as follows:\n",
        "\n",
        "$$\n",
        "\\underset{W}{\\arg \\min}||WX - Y||^2  = \\underset{W}{\\arg \\min} \\left ( ||WX||^2 - 2 \\langle WX, Y \\rangle+ ||Y||^2 \\right ) \\\\\n",
        "= \\underset{W}{\\arg \\min} \\left ( ||X||^2 - 2 \\langle WX, Y \\rangle + ||Y||^2 \\right ) \\\\\n",
        "= \\underset{W}{\\arg \\max}  \\langle WX, Y \\rangle \\\\\n",
        "= \\underset{W}{\\arg \\max}  \\langle W, YX^T \\rangle \n",
        "$$\n",
        "\n",
        "Let  $SVD(YX^T) = U \\Sigma V^T$, where $U$ and $V$ are both orthogonal. Then\n",
        "\n",
        "$$\n",
        "\\underset{W}{\\arg \\max}  \\langle W, YX^T \\rangle = \\underset{W}{\\arg \\max}  \\langle W, U \\Sigma V^T \\rangle \\\\ \n",
        "\\underset{W}{\\arg \\max} \\langle U^T W V, Σ \\rangle\n",
        "$$\n",
        "\n",
        "Given that $U^T W V$ is a product of orthogonal matrices, thus an orthogonal matrix, and also that $Σ$ is diagonal, the maximum of the above expression is obtained by making $U^T W V = I$. Then, we have: \n",
        "\n",
        "$$\n",
        "U^T W V = I \\therefore W = UV^T \n",
        "$$\n",
        "\n",
        "Hence,\n",
        "\n",
        "$$\n",
        "\\underset{W}{\\arg \\min}||WX - Y||^2 = UV^T , \\text{where } SVD(YX^T) = U Σ V^T\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wty1NbzQJ7Ov"
      },
      "source": [
        "Y = target_vocabulary.vectors[y_train, :].T\n",
        "X = source_vocabulary.vectors[X_train, :].T\n",
        "U, sigma, V_t = svd(Y @ X.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LTmbMfUSWng"
      },
      "source": [
        "W_orthogonal = torch.from_numpy(U@V_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma4Pz9xoUhCQ"
      },
      "source": [
        "#**Orthogonal case using cosine distance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SsaEUVkmDK7"
      },
      "source": [
        "Another way to minimize $\n",
        "\\underset{W}{\\arg \\min}||WX - Y||^2 $ , introduced in [Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation](https://aclanthology.org/N15-1104.pdf) , is to use the cosine distance in the transform learning. The optimization\n",
        "task can be redefined as follows: $$ \\max_{W} \\sum_{i} (W_i x_i)^{T}y_i$$\n",
        " A simple calculation shows that the gradient is as follows:$$ \\bigtriangledown W =\\sum_{i} x_i y_i^{T} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS8lrEoC5062"
      },
      "source": [
        "def cos_loss(W, X, y, bs):\n",
        "  return 1-torch.trace((W @ X).T @ y)/bs\n",
        "\n",
        "def cos_grad(W, X, y, bs):\n",
        "  return X@(y.T)/bs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbSBbuO5aI8O"
      },
      "source": [
        "#### **Training**\n",
        "For each epoch, we then:\n",
        "\n",
        "*   Divide the training set in batches\n",
        "*   Perform step for every batch\n",
        "*   Compute loss on training and test sets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzmK59oyzoCv"
      },
      "source": [
        "train_loss=[]\n",
        "test_loss =[]\n",
        "\n",
        "for epoch in range(50):\n",
        "  start, end = 0, batch_size\n",
        "  loss_train, loss_test = 0, 0\n",
        "  i=0\n",
        "  start_time=time.time()\n",
        "  while i < (X_train.shape[0]/batch_size)+1:\n",
        "    X_ = source_vocabulary.vectors[X_train[start:end], :].T\n",
        "    y_ = target_vocabulary.vectors[y_train[start:end], :].T\n",
        "  \n",
        "    W-= learning_rate*grad(W, X_, y_, batch_size)\n",
        "\n",
        "    start = min(start+batch_size, X_train.shape[0]-batch_size)\n",
        "    end = min(end+batch_size, X_train.shape[0])\n",
        "    i+=1\n",
        "\n",
        "  # EVALUATE MODEL\n",
        "\n",
        "  # On the training set\n",
        "  X_ = source_vocabulary.vectors[X_train, :].T\n",
        "  y_ = target_vocabulary.vectors[y_train, :].T\n",
        "\n",
        "  loss_train = loss(W, X_, y_, X_train.shape[0])\n",
        "\n",
        "  # On the test set\n",
        "  X_ = source_vocabulary.vectors[X_test, :].T\n",
        "  y_ = target_vocabulary.vectors[y_test, :].T\n",
        "    \n",
        "  loss_test = loss(W, X_, y_, X_test.shape[0])\n",
        "  \n",
        "  train_loss.append(loss_train)\n",
        "  test_loss.append(loss_test)\n",
        "\n",
        "\n",
        "\n",
        "  print('EPOCH ', epoch, '|| LOSS TRAIN: {:.2f}'.format(loss_train), '|| LOSS TEST:{:.2f}'.format(loss_test) , '|| TIME: {:.2f} '.format(time.time()-start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMS9HrZdwvdH"
      },
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(test_loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Test loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB32oCGE3Lyz"
      },
      "source": [
        "W_2 = torch.nn.init.orthogonal_(torch.empty(300, 300))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qPQfR3U9NeZ"
      },
      "source": [
        "train_loss=[]\n",
        "test_loss =[]\n",
        "\n",
        "\n",
        "for epoch in range(50):\n",
        "  start, end = 0, batch_size\n",
        "  loss_train, loss_test = 0, 0\n",
        "  i=0\n",
        "  start_time=time.time()\n",
        "  while i < (X_train.shape[0]/batch_size)+1:\n",
        "    X_ = source_vocabulary.vectors[X_train[start:end], :].T\n",
        "    y_ = target_vocabulary.vectors[y_train[start:end], :].T\n",
        "  \n",
        "    CG=cos_grad(W_2, X_, y_, batch_size)\n",
        "    W_2 += learning_rate*CG\n",
        "    U, sigma, V = svd(W_2)\n",
        "    W_2 = torch.from_numpy(U@V)\n",
        "\n",
        "    start = min(start+batch_size, X_train.shape[0]-batch_size)\n",
        "    end = min(end+batch_size, X_train.shape[0])\n",
        "    i+=1\n",
        "\n",
        "  # EVALUATE MODEL\n",
        "\n",
        "  # On the training set\n",
        "  X_ = source_vocabulary.vectors[X_train, :].T\n",
        "  y_ = target_vocabulary.vectors[y_train, :].T\n",
        "\n",
        "  loss_train = cos_loss(W_2, X_, y_, X_train.shape[0])\n",
        "\n",
        "  # On the test set\n",
        "  X_ = source_vocabulary.vectors[X_test, :].T\n",
        "  y_ = target_vocabulary.vectors[y_test, :].T\n",
        "    \n",
        "  loss_test = cos_loss(W_2, X_, y_, X_test.shape[0])\n",
        "  \n",
        "  train_loss.append(loss_train)\n",
        "  test_loss.append(loss_test)\n",
        "\n",
        "\n",
        "  print('EPOCH ', epoch, '|| LOSS TRAIN: {:.2f}'.format(loss_train), '|| LOSS TEST:{:.2f}'.format(loss_test) , '|| TIME: {:.2f} '.format(time.time()-start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfoTuVUmji6M"
      },
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(test_loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Test loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK8JazpdXMf-"
      },
      "source": [
        "W_3 = torch.nn.init.orthogonal_(torch.empty(300, 300))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_dvm_ciXMgR"
      },
      "source": [
        "train_loss=[]\n",
        "test_loss =[]\n",
        "beta=0.01\n",
        "for epoch in range(50):\n",
        "  start, end = 0, batch_size\n",
        "  loss_train, loss_test = 0, 0\n",
        "  i=0\n",
        "  start_time=time.time()\n",
        "  while i < (X_train.shape[0]/batch_size)+1:\n",
        "    X_ = source_vocabulary.vectors[X_train[start:end], :].T\n",
        "    y_ = target_vocabulary.vectors[y_train[start:end], :].T\n",
        "  \n",
        "    CG=cos_grad(W_3, X_, y_, batch_size)\n",
        "    W_3 += learning_rate*CG\n",
        "    W_3= (1+beta)*W_3 - beta*(W_3 @ W_3.T)@ W_3\n",
        "\n",
        "\n",
        "    start = min(start+batch_size, X_train.shape[0]-batch_size)\n",
        "    end = min(end+batch_size, X_train.shape[0])\n",
        "    i+=1\n",
        "\n",
        "  # EVALUATE MODEL\n",
        "\n",
        "  # On the training set\n",
        "  X_ = source_vocabulary.vectors[X_train, :].T\n",
        "  y_ = target_vocabulary.vectors[y_train, :].T\n",
        "\n",
        "  loss_train = cos_loss(W_3, X_, y_, X_train.shape[0])\n",
        "\n",
        "  # On the test set\n",
        "  X_ = source_vocabulary.vectors[X_test, :].T\n",
        "  y_ = target_vocabulary.vectors[y_test, :].T\n",
        "    \n",
        "  loss_test = cos_loss(W_3, X_, y_, X_test.shape[0])\n",
        "  \n",
        "  train_loss.append(loss_train)\n",
        "  test_loss.append(loss_test)\n",
        "\n",
        "\n",
        "  print('EPOCH ', epoch, '|| LOSS TRAIN: {:.2f}'.format(loss_train), '|| LOSS TEST:{:.2f}'.format(loss_test) , '|| TIME: {:.2f} '.format(time.time()-start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de2Yrpa-XMgS"
      },
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(test_loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Test loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79FgC_0FcdLA"
      },
      "source": [
        "#### **Evaluation**\n",
        "\n",
        "Next, we'll see the accuracy of the trained model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaI1aqXEVF9n"
      },
      "source": [
        "def get_acc(indices, y_test, k_s=[1, 2, 5, 8, 10, 15]):\n",
        "  accuracies = [0 for i in range(len(k_s))]\n",
        "  # Count accuracy for every k\n",
        "  for i, k in enumerate(k_s):\n",
        "    for j, target_idx in enumerate(y_test):\n",
        "      accuracies[i]+= 1*(target_idx in indices[j, :k])\n",
        "    accuracies[i]= 100* accuracies[i]/ X_test.shape[0]\n",
        "  return accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fk2In9H3ecl"
      },
      "source": [
        "# Perform prediction\n",
        "source_vectors_test = source_vocabulary.vectors[X_test, :].T\n",
        "prediction_test = (W @ source_vectors_test).T\n",
        "prediction_test_orthogonal = (W_orthogonal @ source_vectors_test).T\n",
        "prediction_test_orthogonal_cos = (W_2 @ source_vectors_test).T\n",
        "prediction_test_orthogonal_cos2 = (W_3 @ source_vectors_test).T\n",
        "\n",
        "# Find nearest neighbours\n",
        "tic = time.time()\n",
        "dists, idxs = neighbors_tree.kneighbors(prediction_test.numpy())\n",
        "tac = time.time()\n",
        "print('General case done! Computed ', neighbors_tree.n_neighbors , ' neighbors for ', idxs.shape[0] ,' words in {:.3f}'.format(tac-tic), 's')\n",
        "dists_orthogonal, idxs_orthogonal = neighbors_tree.kneighbors(prediction_test_orthogonal.numpy())\n",
        "print('Orthogonal case done! Computed ', neighbors_tree.n_neighbors , ' neighbors for ', idxs.shape[0] ,' words in {:.3f}'.format(time.time()-tac), 's')\n",
        "tic = time.time()\n",
        "dists_orthogonal_cos, idxs_orthogonal_cos = neighbors_tree.kneighbors(prediction_test_orthogonal_cos.numpy())\n",
        "print('Orthogonal cosine case by SVD done! Computed ', neighbors_tree.n_neighbors , ' neighbors for ', idxs.shape[0] ,' words in {:.3f}'.format(time.time()-tic), 's')\n",
        "tac = time.time()\n",
        "dists_orthogonal_cos2, idxs_orthogonal_cos2 = neighbors_tree.kneighbors(prediction_test_orthogonal_cos2.numpy())\n",
        "print('Orthogonal cosine case by beta done! Computed ', neighbors_tree.n_neighbors , ' neighbors for ', idxs.shape[0] ,' words in {:.3f}'.format(time.time()-tac), 's')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2sIHW7vVJpH"
      },
      "source": [
        "acc = get_acc(idxs, y_test)\n",
        "acc_orthogonal = get_acc(idxs_orthogonal, y_test)\n",
        "acc_orthogonal_cos = get_acc(idxs_orthogonal_cos, y_test)\n",
        "acc_orthogonal_cos2 = get_acc(idxs_orthogonal_cos2, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpehCRs3KEoQ"
      },
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(k_s, acc, color='blue', label='accuracy general case')\n",
        "plt.plot(k_s, acc_orthogonal, color='red', label='accuracy orthogonal case')\n",
        "plt.plot(k_s, acc_orthogonal_cos, color='green', label='accuracy orthogonal cosine case by SVD')\n",
        "plt.plot(k_s, acc_orthogonal_cos2, color='purple', label='accuracy orthogonal cosine case by beta')\n",
        "plt.legend()\n",
        "plt.title('Accuracies for different values of k - English to French')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8D8H4wW3JqT"
      },
      "source": [
        "result = []\n",
        "\n",
        "acc = 0\n",
        "acc_orthogonal = 0\n",
        "acc_cos = 0\n",
        "acc_cos2 = 0\n",
        "prediction_test = (W @ (source_vocabulary.vectors[X_test, :].T)).T\n",
        "prediction_test_cos = (W_2 @ (source_vocabulary.vectors[X_test, :].T)).T\n",
        "prediction_test_cos2 = (W_3 @ (source_vocabulary.vectors[X_test, :].T)).T\n",
        "prediction_test_orthogonal = (W_orthogonal @ (source_vocabulary.vectors[X_test, :].T)).T\n",
        "for i, (english_idx, french_idx) in enumerate(zip(X_test, y_test)):\n",
        "  english_word = source_vocabulary.itos[english_idx]\n",
        "  french_word=target_vocabulary.itos[french_idx]\n",
        "  translation = closest_words(target_vocabulary, prediction_test[i, :] , n = 1)\n",
        "  translation_cos = closest_words_cosin(target_vocabulary, prediction_test_cos[i, :] , n = 1)\n",
        "  translation_cos2 = closest_words_cosin(target_vocabulary, prediction_test_cos2[i, :] , n = 1)\n",
        "  translation_orthogonal = closest_words(target_vocabulary, prediction_test_orthogonal[i, :] , n = 1)\n",
        "\n",
        "  result.append([french_word,english_word, translation[0], translation_orthogonal[0],translation_cos[0],translation_cos2[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIiiFVTr_nTj"
      },
      "source": [
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siiJEQwHVHXw"
      },
      "source": [
        "#### Validate the Test Data\n",
        "for i in range(30):\n",
        "  line_new = '{:<60} traducted : {:>15} (simple SGD) {:>35} (orthogonal) {:>35} (SGD + orthogonal by SVD)  {:>35} (SGD + orthogonal by beta)'.format(color.PURPLE+result[i][1]+color.END+'('+color.GREEN+ result[i][0]+color.END +')',color.YELLOW+ result[i][2] +color.END, color.CYAN+ result[i][3] +color.END, color.RED+ result[i][4] +color.END,color.DARKCYAN+ result[i][5] +color.END)\n",
        "  print(line_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXIFXeOspEjD"
      },
      "source": [
        "#### **Performance visualization on common words**\n",
        "##### English to french case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZk-9VSAl9At"
      },
      "source": [
        "animals_french = ['chien', 'chat', 'cheval', 'vache', 'lion', 'oiseau', 'poisson', 'tigre', 'poule']\n",
        "animals_english = ['dog', 'cat', 'horse', 'cow', 'lion', 'bird', 'fish', 'tiger', 'chicken']\n",
        "\n",
        "animals_french_idx = np.array([target_vocabulary.stoi[animal] for animal in animals_french])\n",
        "animals_english_idx = np.array([source_vocabulary.stoi[animal] for animal in animals_english])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnwYUqHOqXhe"
      },
      "source": [
        "animals_french_vectors = target_vocabulary.vectors[animals_french_idx, :]\n",
        "animals_english_vectors = source_vocabulary.vectors[animals_english_idx, :]\n",
        "animals_english_translation_vectors = (W @ animals_english_vectors.T).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxLgddM_q6gS"
      },
      "source": [
        "pca_french = PCA(n_components=2)\n",
        "pca_english = PCA(n_components=2)\n",
        "pca_english_translated = PCA(n_components=2) \n",
        "\n",
        "french_2D = pca_french.fit_transform(animals_french_vectors)\n",
        "\n",
        "english_2D = pca_english.fit_transform(animals_english_vectors)\n",
        "\n",
        "english_translated_2D = pca_english_translated.fit_transform(torch.cat([animals_french_vectors, animals_english_translation_vectors], 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QNXh_sAq9T0"
      },
      "source": [
        "labels = animals_french + animals_english\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "ax.scatter(english_translated_2D[:len(animals_french),0], english_translated_2D[:len(animals_french),1], color='blue', label='french words')\n",
        "ax.scatter(english_translated_2D[len(animals_french):,0], english_translated_2D[len(animals_french):,1], color='red', label='english words translated')\n",
        "\n",
        "plt.legend()\n",
        "for i, txt in enumerate(labels):\n",
        "    ax.annotate(txt, (english_translated_2D[i][0], english_translated_2D[i][1]))\n",
        "\n",
        "plt.title('Comparison between french words and translated english words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G85p5ZaGukK"
      },
      "source": [
        "#### **Unsupervised Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGqn48QZtq_F"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(300, 500),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.BatchNorm1d(500),\n",
        "        nn.Linear(500, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "  \n",
        "class Generator(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    self.W = nn.Linear(300, 300, bias=False)\n",
        "    self.betha = 0.01\n",
        "    nn.init.eye_(self.W.weight.data)\n",
        "  \n",
        "  def keep_orthogonal(self):\n",
        "    self.W.weight.data.copy_((1+self.betha)*self.W.weight.data - self.betha * (self.W.weight.data @ (self.W.weight.data.T)) @ (self.W.weight.data))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.W(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCjdehHaUNSS"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "generator = generator.to(device)\n",
        "discriminator = discriminator.to(device)\n",
        "\n",
        "criterion_generator = nn.BCEWithLogitsLoss()\n",
        "criterion_discriminator = nn.BCEWithLogitsLoss()\n",
        "optimizer_generator = torch.optim.SGD(generator.parameters(), lr=0.01, weight_decay=0.98)\n",
        "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=0.05,   betas=(0.5, 0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWZOVUjlLmrx"
      },
      "source": [
        "num_epochs = 6\n",
        "batch_size = 32\n",
        "source_embeddings = nn.Embedding.from_pretrained(source_vocabulary.vectors).to(device)\n",
        "target_embeddings = nn.Embedding.from_pretrained(target_vocabulary.vectors).to(device)\n",
        "vocab_size = len(source_vocabulary.itos)\n",
        "epoch_size = 2*int(vocab_size/batch_size)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # TRAIN\n",
        "  i=0\n",
        "  loss=0\n",
        "  gen=0\n",
        "\n",
        "  for i in range(epoch_size):\n",
        "\n",
        "    source_idx = torch.randint(0, vocab_size, (batch_size,)).to(device) # generate ramdom indices\n",
        "    \n",
        "    vectors_en = source_embeddings(source_idx)\n",
        "    \n",
        "    ### TRAIN DISCRIMINATOR ###\n",
        "    \n",
        "    # WITH FAKE EXAMPLES (GENERATED BY GENERATOR)\n",
        "    for j in range(3):\n",
        "      discriminator.train()\n",
        "      generator.eval()\n",
        "      \n",
        "      discriminator.zero_grad()\n",
        "      optimizer_discriminator.zero_grad()\n",
        "\n",
        "      vectors_fake = generator(vectors_en)\n",
        "      discriminator_labels_fake = discriminator(vectors_fake.data.detach())\n",
        "\n",
        "      labels_fake = (torch.randn_like(discriminator_labels_fake)*0.2).to(device)  #USING NOISY LABELS\n",
        "\n",
        "      loss_discriminator_fake = criterion_discriminator(discriminator_labels_fake, labels_fake).to(device)\n",
        "      D_G_z1 = discriminator_labels_fake.cpu().mean().item()\n",
        "\n",
        "      # WITH TRUE EXAMPLES\n",
        "      target_idx = torch.randint(0, vocab_size, (batch_size,)).to(device)\n",
        "      \n",
        "      vectors_true = target_embeddings(target_idx)\n",
        "\n",
        "      discriminator_labels_true = discriminator(vectors_true)\n",
        "      labels_true = (torch.randn_like(discriminator_labels_true)*0.2 + 1.).to(device)\n",
        "\n",
        "      loss_discriminator_true = criterion_discriminator(discriminator_labels_true, labels_true).to(device)\n",
        "      \n",
        "\n",
        "      loss_discriminator = 0.5*(loss_discriminator_fake + loss_discriminator_true)\n",
        "      loss_discriminator.backward()\n",
        "      D_x = discriminator_labels_true.cpu().mean().item()\n",
        "      optimizer_discriminator.step()\n",
        "\n",
        "    ### TRAIN GENERATOR ###\n",
        "    discriminator.eval()\n",
        "    generator.train()\n",
        "\n",
        "    target_idx = torch.randint(0, vocab_size, (batch_size,)).to(device)\n",
        "    generator.zero_grad()\n",
        "    optimizer_generator.zero_grad()\n",
        "    vectors_en = target_embeddings(target_idx)\n",
        "\n",
        "    vectors_fake = generator(vectors_en)\n",
        "    discriminator_labels_fake = discriminator(vectors_fake)\n",
        "    labels_true = (torch.randn_like(discriminator_labels_fake)*0.2 + 1.).to(device)\n",
        "\n",
        "    loss_generator = criterion_generator(discriminator_labels_fake, labels_true)\n",
        "\n",
        "    D_G_z2 = discriminator_labels_fake.cpu().mean().item()\n",
        "\n",
        "    loss_generator.backward()\n",
        "    optimizer_generator.step()\n",
        "    generator.keep_orthogonal()\n",
        "\n",
        "      # Output training stats\n",
        "    if i % 100 == 0 and i!=0:\n",
        "        print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "              % (epoch, num_epochs, i, epoch_size,\n",
        "                  loss_discriminator.item(), loss_generator.item(), D_x, D_G_z1, D_G_z2))\n",
        "        loss=0\n",
        "        gen=0\n",
        "    i+=1\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stMJIrMcVsO_"
      },
      "source": [
        "### **Check accuracy without procrustes refinement**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL3na9JFeG9O"
      },
      "source": [
        "prediction_test_gan = generator(source_vocabulary.vectors[X_test, :].to(device)).cpu()\n",
        "tic = time.time()\n",
        "dists_gan, idxs_gan = neighbors_tree.kneighbors(prediction_test_gan.detach().numpy())\n",
        "print('GAN case done! Computed ', neighbors_tree.n_neighbors , ' neighbors for ', idxs_gan.shape[0] ,' words in {:.3f}'.format(time.time()-tic), 's')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw5K7ANDV4cg"
      },
      "source": [
        "acc_gan = get_acc(idxs_gan, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oXze1sdWJ3F"
      },
      "source": [
        "#### Replot to compare with supervised methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T04g0SAKspzY"
      },
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "if acc:\n",
        "  plt.plot(k_s, acc, color='blue', label='accuracy general case')\n",
        "if acc_orthogonal:\n",
        "  plt.plot(k_s, acc_orthogonal, color='red', label='accuracy orthogonal case')\n",
        "\n",
        "plt.plot(k_s, acc_gan, color='green', label='accuracy gan')\n",
        "plt.legend()\n",
        "plt.title('Accuracies for different values of k - English to French')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0yPfccPWeTO"
      },
      "source": [
        "#### **PROCRUSTES REFINEMENT**\n",
        "\n",
        "To  perform procrustes refinement, we'll first select the closest predictions made by our generator. These predictions are considered to be accurate and common. We'll then construct our dictionary only with them, and we'll pergorm the orthogonal solution. We'll use a threshold for the distances, here we take the median of the distances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKZr94xXXsps"
      },
      "source": [
        "# We'll first look for the nearest neighbors in the training set.\n",
        "prediction_train_gan = generator(source_vocabulary.vectors[X_train, :].to(device)).cpu()\n",
        "tic=time.time()\n",
        "dists_train_gan, idxs_train_gan = neighbors_tree.kneighbors(prediction_train_gan.detach().numpy(), n_neighbors=1)\n",
        "print('1-NN for dictionary computed for ', idxs_train_gan.shape[0] ,' words in {:.3f}'.format(time.time()-tic), 's')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU2EhemrsxgK"
      },
      "source": [
        "median = np.median(dists_train_gan)\n",
        "indices_translation = torch.tensor([[j, i[0]] for d, i, j in zip(dists_train_gan, idxs_train_gan, X_train) if d < median])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYN4hM2RYkGa"
      },
      "source": [
        "Once we have the words from source and target space that are next to each other by the leasts distances, we can use them to compute our Procrustes solution. This will be our new weights for the generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO6_gzGDu2ZS"
      },
      "source": [
        "src_idx = indices_translation[:, 0]\n",
        "tgt_idx = indices_translation[:, 1]\n",
        "\n",
        "closest_words_source = source_embeddings.weight.data[src_idx.to(device)]\n",
        "closest_words_target = target_embeddings.weight.data[tgt_idx.to(device)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4brQInSwe2Z"
      },
      "source": [
        "M = closest_words_target.transpose(0, 1).mm(closest_words_source).cpu().numpy()\n",
        "U_, _, V_T = svd(M, full_matrices=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62YCzd1CzUq0"
      },
      "source": [
        "new_generator = Generator()\n",
        "new_generator=new_generator.to(device)\n",
        "new_generator.W.weight.data.copy_(torch.from_numpy(U_.dot(V_T)).type_as(generator.W.weight.data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2PWh5aGZQDy"
      },
      "source": [
        "#### Final evaluation\n",
        "\n",
        "We can finally evaluate our last solution that uses gan+procrustes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlpR4_jkVCT5"
      },
      "source": [
        "prediction_test = new_generator(source_vocabulary.vectors[X_test, :].to(device)).cpu()\n",
        "tic = time.time()\n",
        "dists_unsupervised, idxs_unsupervised = neighbors_tree.kneighbors(prediction_test.detach().numpy())\n",
        "print('Unsupervised (GAN+Procrustes) case done! Computed ', neighbors_tree.n_neighbors , ' neighbors for ', idxs_unsupervised.shape[0] ,' words in {:.3f}'.format(time.time()-tic), 's')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgInMJe81Xn0"
      },
      "source": [
        "acc_unsupervised = get_acc(idxs_unsupervised, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfr5qFVHcXq2"
      },
      "source": [
        "### FINAL PLOT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSnmNIWJcXIm"
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "if acc:\n",
        "  plt.plot(k_s, acc, color='blue', label='accuracy general case')\n",
        "if acc_orthogonal:\n",
        "  plt.plot(k_s, acc_orthogonal, color='red', label='accuracy orthogonal case')\n",
        "if acc_gan:\n",
        "  plt.plot(k_s, acc_gan, color='green', label='accuracy gan')\n",
        "\n",
        "plt.plot(k_s, acc_unsupervised, color='purple', label='accuracy unsupervised method')\n",
        "plt.legend()\n",
        "plt.title('Accuracies for different values of k - English to French')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}